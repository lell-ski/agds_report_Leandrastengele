---
title: "Digital Soil Mapping"
author: "Leandra Stengele"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## initiating Git
```{r}
usethis::use_git()    
```


```{r}
library(readr)
library(knitr)
library(here)
library(Boruta)
```


FOLGENDER CODE FUNKTIONIERT NICHT, WEIL WIR DAS .CSV NICHT HABEN! AB KAP. 4 HABEN WIR ALLES UND AB DA MACHT ES SINN DEN CODE RÜBER ZU KOPIEREN. ALLES VORHER IS DATA WRANGLING
```{r}
df_full <- readRDS(here::here("data/df_full.rds"))

head(df_full) |> 
  knitr::kable()
```
## 3.2 Preparations
- Specify target and predictor variables (predictors: only vbls we have spatial coverage)
- split dataset into training and testing set, remove NA
```{r}
# Specify target: The pH in the top 10cm
target <- "ph.0.10"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> dplyr::filter(dataset == "calibration")
df_test  <- df_full |> dplyr::filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> tidyr::drop_na()
df_test <- df_test   |> tidyr::drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

# save target and predictor vale for easy access later
perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```

## 3.3 Model training
The modelling task is to predict the soil pH in the top 10 cm. Let’s start using the default hyperparameters used by ranger::ranger().

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 42,                    # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```
Although we only used the pre-defined parameters, we already get a fairly good out-of-bag (OOB) of 0.45 and a MSE of 0.32 pH units. 
*This is the step at which you may want to reduce the number of predictors_all to avoid collinearity and the risk of overfitting. *

## 3.4 Variable importance
```{r}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all],   # Predictor variables
  importance   = "permutation", # Pick permutation to calculate variable importance
  seed = 42,                    # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

# Display plot
gg
```
We find that the mean annual precipitation is by far the most important variable in determining soil pH in our model.

## 3.5 Variable selection

the “Boruta-Algorithm” - an effective and popular approach to variable selection
```{r}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 30, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

```{r}
# get retained important variables
predictors_selected <- df_bor |> 
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger::ranger( 
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```
Save the model object with the reduced set of predictors_all, calibration, and validation data for the subsequent Chapter.
```{r}
# Save relevant data for model testing in the next chapter.
saveRDS(rf_bor,                   
        here::here("data/rf_for_ph0-10.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_ph0-10.rds"))

saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_ph0-10.rds"))
```

### 4 Model Analysis
## 4.1 Load model and data
```{r}
# Load random forest model
rf_bor   <- readRDS(here::here("data/rf_for_pH0-10.rds"))
df_train <- readRDS(here::here("data/cal_for_ph0-10.rds"))
df_test  <- readRDS(here::here("data/val_for_ph0-10.rds"))
```

```{r}
# Load area to be predicted
raster_mask <- terra::rast(here::here("data-raw/geodata/study_area/area_to_be_mapped.tif"))

# Turn target raster into a dataframe, 1 px = 1 cell
df_mask <- as.data.frame(raster_mask, xy = TRUE)

# Filter only for area of interest
df_mask <- df_mask |> 
  dplyr::filter(area_to_be_mapped == 1)

# Display df
head(df_mask) |> 
  knitr::kable()
```




