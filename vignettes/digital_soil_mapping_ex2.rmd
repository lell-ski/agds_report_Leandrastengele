---
title: "Digital Soil Mapping"
author: "Leandra Stengele"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(knitr)
library(here)
library(Boruta)
library(caret)
library(pROC)
```

```{r}
df_full <- readRDS(here::here("data/df_full.rds"))

head(df_full) |> 
  knitr::kable()
```

# 3 Digital Soil mapping (AGDS workflow)

## 3.2 Preparations

-   Specify target and predictor variables (predictors: only vbls we have spatial coverage)

```{r}
# Specify target: The pH in the top 10cm (that we wanna predict in the end)
target <- "ph.0.10"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```

-   split dataset into training and testing set, remove NA

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> dplyr::filter(dataset == "calibration")
df_test  <- df_full |> dplyr::filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> tidyr::drop_na()
df_test <- df_test   |> tidyr::drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

# save target and predictor vale for easy access later
perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```

## 3.3 Model training

The modelling task is to predict the soil pH in the top 10 cm. Let’s start using the default hyperparameters used by ranger::ranger().

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 42,                    # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```

Although we only used the pre-defined parameters, we already get a fairly good out-of-bag (OOB) of 0.45 and a MSE of 0.32 pH units. *This is the step at which you may want to reduce the number of predictors_all to avoid collinearity and the risk of overfitting.*

## 3.4 Variable importance

```{r fig.height=18}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all],   # Predictor variables
  importance   = "permutation", # Pick permutation to calculate variable importance
  seed = 42,                    # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

# Display plot
gg
```

We find that the mean annual precipitation is by far the most important variable in determining soil pH in our model.

## 3.5 Variable selection

the “Boruta-Algorithm” - an effective and popular approach to variable selection

```{r fig.height=18}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 30, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

```{r}
# get retained important variables
predictors_selected <- df_bor |> 
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger::ranger( 
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```

Save the model object with the reduced set of predictors_all, calibration, and validation data for the subsequent Chapter.

```{r}
# Save relevant data for model testing in the next chapter.
saveRDS(rf_bor,                   
        here::here("data/rf_for_ph0-10.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_ph0-10.rds"))

saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_ph0-10.rds"))
```

# 5 Report Exercise

## 5.1 Simple model

-   Specify target and predictor variables (predictors: only vbls we have spatial coverage)

```{r}
# Specify target: waterlog
target_wa <- "waterlog.100"

# Make sure that the categorical target variable is encoded as a factor using the function factor()
df_full[[target_wa]] <- factor(df_full[[target_wa]])

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target_wa,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```

-   split dataset into training and testing set, remove NA

```{r}
# Split dataset into training and testing sets
df_train_wa <- df_full |> dplyr::filter(dataset == "calibration")
df_test_wa  <- df_full |> dplyr::filter(dataset == "validation")

# Make sure that the categorical target variable is encoded as a factor using the function factor()
df_train_wa[[target_wa]] <- factor(df_train_wa[[target_wa]])
df_test_wa[[target_wa]]  <- factor(df_test_wa[[target_wa]])

# Filter out any NA to avoid error when running a Random Forest
df_train_wa <- df_train_wa |> tidyr::drop_na()
df_test_wa <- df_test_wa   |> tidyr::drop_na()


# A little bit of verbose output:
n_tot_wa <- nrow(df_train_wa) + nrow(df_test_wa)

# save target and predictor vale for easy access later
perc_cal_wa <- (nrow(df_train_wa) / n_tot_wa) |> round(2) * 100
perc_val_wa <- (nrow(df_test_wa)  / n_tot_wa) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal_wa, "/", perc_val_wa, "%")
```

## Model training

The modelling task is to predict the soil pH in the top 10 cm. Let’s start using the default hyperparameters used by ranger::ranger().

```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic_wa <- ranger::ranger( 
  y = df_train_wa[, target_wa, drop = T],     # target variable; drop = T to ensure i get Vectors
  x = df_train_wa[, predictors_all], # Predictor variables
  seed = 42,                    # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic_wa)
```

**Comment: The OOB prediction error improved compared to the basic model without the inclusion of waterlogged information. From 32.1% to 21.3%.**

## Evaluate simple model

(Ch. 9.4.1.2)

```{r}
# Predictions on test set
pred_test_wa <- predict(              # predict() returns a prediction obj (not just a vector), which contains i.a. $predictions which is made of 0 (not waterlogged) and 1 (waterlogged)
  rf_basic_wa,                        # the model (my model) that ranger applies
  data = df_test_wa[, predictors_all] # input data are x (pred) from the test dataset
)


## EVALUATION
# store the waterlog predicitons (0/1) in a separate obj
pred_class_wa <- pred_test_wa$predictions 

# extract the true observed values (0/1) from the test dataset
obs_class_wa <- df_test_wa[[target_wa]] # [[ ]] to extract the column as a vector (not as a 1-column df)

# Confusion matrix
cm_wa <- table(observed = obs_class_wa, predicted = pred_class_wa)
cm_wa
#    0  1
# 0 TN FP
# 1 FN TP

TP <- cm_wa["1", "1"]
FN <- cm_wa["1", "0"]
FP <- cm_wa["0", "1"]
TN <- cm_wa["0", "0"]

# calculate metrics
accuracy_wa  <- (TP + TN) / (TP + TN + FP + FN)
precision_wa <- TP / (TP + FP)
TPR_wa       <- TP / (TP + FN)
FPR_wa       <- FP / (FP + TN)

metrics_wa <- c(
  accuracy_wa = accuracy_wa,
  precision_wa = precision_wa,
  TPR_wa = TPR_wa,
  FPR_wa = FPR_wa
  
)
metrics_wa
```

*Interpretation: In the confusion matrix we see 70 observed values (22+48) that are waterlogged (TRUE) and 130 observations (105+25) that are not waterlogged (FALSE). This is 35% True vs. 65% False. So TRUE and FALSE are not well balanced. This imbalance restricts the comparison of our accuarcy with a random model with acccuracy 0.5 because this would assume that both cases (T/F) are equally likely to appear. Just predicting the majority class would already lead to an accuracy of 65% and our model reaches 76.5% which isn't that much higher. The precision tells us that in 65.7% of predicted waterlogged (true), it is correct. The TPR tells us that among the observed waterlogged sites, we detected 68.5% correctly. From FPR we know that we marked 19% of the sites that are actually not waterlogged (F) incorrectly as waterlogged (T). However, it is important to state that due to the imbalance in observed T/F we should be careful when interpreting the metrics, especially when making interpretations about the True (waterlogged) class, since it is much rarer than the False class.*

## 5.2 Variable selection

### Variable importance

```{r fig.height=18}
# Let's run the basic model again but with recording the variable importance
rf_basic_wa <- ranger::ranger( 
  y = df_train_wa[, target_wa],     # target variable
  x = df_train_wa[, predictors_all],   # Predictor variables
  importance   = "permutation", # Pick permutation to calculate variable importance
  seed = 42,                    # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic_wa <- rf_basic_wa$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg_wa <- vi_rf_basic_wa |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

# Display plot
plot(gg_wa)
```

### Variable (predictor) selection

the “Boruta-Algorithm” - an effective and popular approach to variable selection

```{r fig.height=18}
set.seed(42)

# run the algorithm
bor_wa <- Boruta::Boruta(
    y = df_train_wa[, target_wa], 
    x = df_train_wa[, predictors_all],
    maxRuns = 30, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor_wa <- Boruta::attStats(bor_wa) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor_wa) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

```{r}
# get retained important variables
predictors_selected_wa <- df_bor_wa |> 
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

length(predictors_selected_wa)
```

```{r}
# re-train Random Forest model
rf_wa_bor <- ranger::ranger( 
  y = df_train_wa[, target_wa],              # target variable
  x = df_train_wa[, predictors_selected_wa], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_wa_bor
```

### Repeat the model evaluation (Ch. 9.4.1.2)

```{r}
# Predictions on test set
pred_test_wa_bor <- predict(              # predict() returns a prediction obj (not just a vector), which contains i.a. $predictions which is made of 0 (not waterlogged) and 1 (waterlogged)
  rf_wa_bor,                        # the model (my model) that ranger applies
  data = df_test_wa[, predictors_selected_wa] # input data are x (pred) from the test dataset
)


## EVALUATION
# store the waterlog predicitons (0/1) in a separate obj
pred_class_wa_bor <- pred_test_wa_bor$predictions 

# extract the true observed values (0/1) from the test dataset
obs_class_wa <- df_test_wa[[target_wa]] # [[ ]] to extract the column as a vector (not as a 1-column df)

# Confusion matrix
cm_wa_bor <- table(observed = obs_class_wa, predicted = pred_class_wa_bor)
cm_wa_bor
#    0  1
# 0 TN FP
# 1 FN TP

TP_bor <- cm_wa_bor["1", "1"]
FN_bor <- cm_wa_bor["1", "0"]
FP_bor <- cm_wa_bor["0", "1"]
TN_bor <- cm_wa_bor["0", "0"]

# calculate metrics
accuracy_wa_bor  <- (TP_bor + TN_bor) / (TP_bor + TN_bor + FP_bor + FN_bor)
precision_wa_bor <- TP_bor / (TP_bor + FP_bor)
TPR_wa_bor       <- TP_bor / (TP_bor + FN_bor)
FPR_wa_bor       <- FP_bor / (FP_bor + TN_bor)

metrics_wa_bor <- c(
  accuracy_wa_bor = accuracy_wa_bor,
  precision_wa_bor = precision_wa_bor,
  TPR_wa_bor = TPR_wa_bor,
  FPR_wa_bor = FPR_wa_bor
  
)
metrics_wa_bor
```

### Interpretation 5.2

```{r}
# comparing metrics
metrics_wa
metrics_wa_bor
```

**Which model generalises better to unseen data?**

*When comparing the test performance, the boruta model generalises better. Though only slightly, all the four metrics improved after the variable selection. The reduction of predictors, in this case, led to more accurate results. The model detected more sites correctly as waterlogged and not waterlogged.*

```{r}
# comparing OOB
rf_basic_wa
rf_wa_bor
```

**Would the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?**

*From the basic model to the reduced, boruta, model the OOB improved from 21.32% to 20.99%. Also here, the difference is small but seeing that it still improved a bit, i would again prefer the boruta model.*

## 5.3 Model optimization

### 5.3.1 5-fold cross-validation

```{r}
pp_wa <- recipes::recipe(
  stats::as.formula(
    paste(target_wa, "~", paste(predictors_selected_wa, collapse = " + "))), # instead of writing out all 37 selected predictors (target_wa~p1+p2+...+p37)
  data = df_train_wa
) |>
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())


set.seed(42)
mod_cv_wa <- caret::train(
  pp_wa,
  data = df_train_wa |> tidyr::drop_na(),
  method = "ranger",
  trControl = caret::trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid(
    .mtry = c(4, floor(sqrt(length(predictors_selected_wa))), 8), # floor(sqrt(length(predictors_selected_wa))) is what the agds book suggests (sqrt(p))
    .min.node.size = c(1, 3, 5, 8, 10, 15, 20),  # 1 = danger of overfit
    .splitrule = "gini"       # classification
  ),
  metric = "Accuracy",        # loss function for classification
  num.trees = 2000,            # enough trees for tuning
  seed = 42                 # reproducibility, like in the book
)
ggplot(mod_cv_wa)
```

*Comment: For the optimization of mtry, the values c(4, sqrt(p), 8) were used where sqrt(p) \~= 6 is the approach the agds book proposes and 4 and 8 were added to have a comparison. For the min.node.size the values c(1, 3, 5, 8, 10, 15, 20) were chosen for optimization. Splitrule = "gini" and metric = "Accuracy" were used for the classification purpose. The final values used for the model with 2000 trees were mtry = 6, splitrule = gini and min.node.size = 8. The accuracy of this best combination is 0.7982814. In the most recent plot mtry = 6 has the overall best performance but there isn't a clear pattern. Also, the mtry = 4 and 8 show no clear pattern. This leads to the assumption that the model is not very sensitive to the number of predictors selected. However, it can still be concluded that mtry=6 seems to be the best choice for most node sizes at least in most of the cases.*

### 5.3.2 Evaluation

```{r}
# Predictions on the test set
pred_test_cv_wa <- predict(
  mod_cv_wa,
  newdata = df_test_wa[, predictors_selected_wa]
)

# store predictions (factor "0"/"1")
pred_class_cv_wa <- pred_test_cv_wa

# observed test classes
obs_class_cv_wa <- df_test_wa[[target_wa]]

# Confusion matrix
cm_cv_wa <- table(observed = obs_class_cv_wa, predicted = pred_class_cv_wa)
cm_cv_wa
#      predicted
# observed   0   1
#        0  TN  FP
#        1  FN  TP

# extract TP, FP, TN, FN (class "1" = waterlogged)
TP_cv_wa <- cm_cv_wa["1", "1"]
FN_cv_wa <- cm_cv_wa["1", "0"]
FP_cv_wa <- cm_cv_wa["0", "1"]
TN_cv_wa <- cm_cv_wa["0", "0"]

# metrics
accuracy_cv_wa  <- (TP_cv_wa + TN_cv_wa) / (TP_cv_wa + TN_cv_wa + FP_cv_wa + FN_cv_wa)
precision_cv_wa <- TP_cv_wa / (TP_cv_wa + FP_cv_wa)
TPR_cv_wa       <- TP_cv_wa / (TP_cv_wa + FN_cv_wa)          # sensitivity / recall
FPR_cv_wa       <- FP_cv_wa / (FP_cv_wa + TN_cv_wa)

metrics_cv_wa <- c(
  accuracy_cv_wa  = accuracy_cv_wa,
  precision_cv_wa = precision_cv_wa,
  TPR_cv_wa       = TPR_cv_wa,
  FPR_cv_wa       = FPR_cv_wa
)

print(metrics_cv_wa)   # optimized
print(metrics_wa_bor)  # before optimization
print(metrics_wa)      # initial default model

```

```{r}
# get a better overview to compare the models
metrics_all <- rbind(
  optimized = unname(metrics_cv_wa),
  boruta   = unname(metrics_wa_bor),
  basic  = unname(metrics_wa)
)

colnames(metrics_all) <- c("accuracy", "precision", "TPR", "FPR")

round(metrics_all, 3)

```

*When comparing the test performances, the optimized model generalises best compared to the boruta model and the basic model before variable selection (both with default parameters). However, the optimized model improves only very slightly compared to the boruta model with default parameters. In this case it is only one true observation more that was correctly detected by the model. So, when looking strictly at the metrics, we would say the optimized generalises better than the boruta model with default parameters.*

## 5.4 Probabilistic predictions

### 5.4.1 Train model

```{r}
set.seed(42)
rf_wa_prob <- ranger::ranger(
  y = df_train_wa[, target_wa],
  x = df_train_wa[, predictors_selected_wa],
  mtry          = 6,
  min.node.size = 8,
  num.trees     = 200,
  probability   = TRUE,
  seed          = 42
)

# Probabilistic predictions on test set
pred_rf_wa_prob <- predict(
  rf_wa_prob,
  data = df_test_wa[, predictors_selected_wa]
)$predictions   # matrix with columns "0" and "1"

# Probability that waterlog.100 == 1 (waterlogged)
prob_wa <- pred_rf_wa_prob[, "1"]

# Observed test labels
obs_wa <- df_test_wa[[target_wa]]  # factor("0","1")
```

```{r}
# other threshold
threshold <- 0.3

pred_class_thresh <- ifelse(prob_wa >= threshold, "1", "0") |>
  factor(levels = c("0", "1"))

cm_thresh <- table(observed = obs_wa, predicted = pred_class_thresh)
cm_thresh

```

### 5.4.2 ROC Curve

```{r}
roc_wa <- pROC::roc(response = obs_wa, predictor = prob_wa)


plot(
  roc_wa,
  legacy.axes = TRUE,          # x-axis = 1 - specificity (i.e. FPR), 0 -> 1
  xlab = "False positive rate",
  ylab = "True positive rate",
  main = "ROC curve for waterlog")

# Area under the curve
auc_wa <- pROC::auc(roc_wa)
auc_wa

```

*Choice of threshold:*

*a) for the infrastructure construction project predicting non-waterlogged when it actually is waterlogged (false negative, prediction: 0, reality: 1) would have severe consequences. On the other hand it is less severe if the model predicts waterlogged but it actually isn't (false positives, prediction: 1, reality: 0). In this case, I would choose a lower threshold so that the cases that are hard to predict (around 0.5) get assigned rather to waterlogged (= 1) to minimize false negatives. This would increase both TPR and FPR and with that also the false alarms but on the other hand I catch more true waterlogged sites which is crucial in this case. The threshold could be e.g. around 0.3.*

*b) For the project where waterlogged are unwanted but not crucial, i would select a threshold that focuses maximising correct outputs. then i might have more FP compared to a) but since it is not as critical i accept that as a trade of to maximize overall accuracy. So, the threshold could be around 0.5 or even higher if that results in a higher accuracy.*

*c) analogy: for example in medical uses, if we are talking about catching a severe illness. then you would want to have a lower threshold and rather mark someone as ill so that they get treated for sure or further examined rather than the illness is not detected and worsens until it's too late to treat. On the other hand you might use a higher threshold if a treatment is very invasive or expensive and a disease is not severe or treatment is still equally effective in a later station.*
